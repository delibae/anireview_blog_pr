{"cells":[{"cell_type":"markdown","metadata":{"id":"cemfWUyAH4uE"},"source":["# 깃허브 연동"]},{"cell_type":"markdown","metadata":{"id":"RAiXBGp9dP2u"},"source":["기본 연동"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22029,"status":"ok","timestamp":1665258609157,"user":{"displayName":"배한진","userId":"03474339660030161101"},"user_tz":-540},"id":"KFFXN6sf85An","outputId":"59ef43f4-0689-4440-d69c-cb9c8a431fda"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":520,"status":"ok","timestamp":1665258612214,"user":{"displayName":"배한진","userId":"03474339660030161101"},"user_tz":-540},"id":"IoN3Pvkl9ZFS","outputId":"fd2446d8-d215-4a11-a7d1-f534d4b10158"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/github_colab\n"]}],"source":["cd /content/drive/MyDrive/github_colab"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1665258614086,"user":{"displayName":"배한진","userId":"03474339660030161101"},"user_tz":-540},"id":"PodAG5cD95OS","outputId":"1fe5a3ed-ae63-4c7e-b824-4afc79a0a58f"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/github_colab/anireview_blog_pr\n"]}],"source":["cd anireview_blog_pr/"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3605,"status":"ok","timestamp":1665258619247,"user":{"displayName":"배한진","userId":"03474339660030161101"},"user_tz":-540},"id":"5aU_pbKb99un"},"outputs":[],"source":["!git config --global user.email 'iwdhanjin@gmail.com'\n","!git config --global user.name 'delibae'"]},{"cell_type":"markdown","metadata":{"id":"221MhLdKc-oY"},"source":["커밋과정"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nA4stPjC-NVz"},"outputs":[],"source":["!git add .\n","!git commit -m \"reveiw classification model created\"\n","!git push"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rXmeSWH0-TuR"},"outputs":[],"source":["!git commit -m \"test\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9WFdLTW7-YzR"},"outputs":[],"source":["!git push"]},{"cell_type":"markdown","metadata":{"id":"yiFhUVUpdFii"},"source":["pull 과정"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1191,"status":"ok","timestamp":1665255368434,"user":{"displayName":"배한진","userId":"03474339660030161101"},"user_tz":-540},"id":"HCpT8LhDdHh0","outputId":"969eaf2f-6ab4-4d1d-865d-970d8700217c"},"outputs":[{"output_type":"stream","name":"stdout","text":["remote: Enumerating objects: 9, done.\u001b[K\n","remote: Counting objects:  11% (1/9)\u001b[K\rremote: Counting objects:  22% (2/9)\u001b[K\rremote: Counting objects:  33% (3/9)\u001b[K\rremote: Counting objects:  44% (4/9)\u001b[K\rremote: Counting objects:  55% (5/9)\u001b[K\rremote: Counting objects:  66% (6/9)\u001b[K\rremote: Counting objects:  77% (7/9)\u001b[K\rremote: Counting objects:  88% (8/9)\u001b[K\rremote: Counting objects: 100% (9/9)\u001b[K\rremote: Counting objects: 100% (9/9), done.\u001b[K\n","remote: Compressing objects:  50% (1/2)\u001b[K\rremote: Compressing objects: 100% (2/2)\u001b[K\rremote: Compressing objects: 100% (2/2), done.\u001b[K\n","Unpacking objects:  20% (1/5)   \rUnpacking objects:  40% (2/5)   \rUnpacking objects:  60% (3/5)   \rUnpacking objects:  80% (4/5)   \rremote: Total 5 (delta 2), reused 5 (delta 2), pack-reused 0\u001b[K\n","Unpacking objects: 100% (5/5)   \rUnpacking objects: 100% (5/5), done.\n","From https://github.com/delibae/anireview_blog_pr\n","   7176297..478a9f9  master     -> origin/master\n","Updating 7176297..478a9f9\n","Fast-forward\n"," ml/ml_data/training.xlsx | Bin \u001b[31m317005\u001b[m -> \u001b[32m827182\u001b[m bytes\n"," 1 file changed, 0 insertions(+), 0 deletions(-)\n"]}],"source":["!git pull"]},{"cell_type":"markdown","metadata":{"id":"10F0uvo7d0Iw"},"source":["# 준비사항"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8939,"status":"ok","timestamp":1665244603919,"user":{"displayName":"배한진","userId":"03474339660030161101"},"user_tz":-540},"id":"b4nInTrkd4xE","outputId":"6b1b4bd1-6fa4-47e8-e117-f031eb136f93"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.22.2-py3-none-any.whl (4.9 MB)\n","\u001b[K     |████████████████████████████████| 4.9 MB 4.8 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 59.2 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Collecting huggingface-hub<1.0,>=0.9.0\n","  Downloading huggingface_hub-0.10.0-py3-none-any.whl (163 kB)\n","\u001b[K     |████████████████████████████████| 163 kB 69.0 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (5.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.10.0 tokenizers-0.12.1 transformers-4.22.2\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rog08DNpeoqR"},"outputs":[],"source":["import tensorflow as tf\n","import torch\n","\n","from transformers import BertTokenizer\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","from transformers import get_linear_schedule_with_warmup\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","\n","import pandas as pd\n","import numpy as np\n","import random\n","import time\n","import datetime"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":355,"status":"ok","timestamp":1665244627114,"user":{"displayName":"배한진","userId":"03474339660030161101"},"user_tz":-540},"id":"ImBtAkSyTW1r","outputId":"58989e29-5ca6-4e9f-f392-b87d30413211"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'nsmc' already exists and is not an empty directory.\n"]}],"source":["# 네이버 영화리뷰 감정분석 데이터 다운로드\n","!git clone https://github.com/e9t/nsmc.git"]},{"cell_type":"markdown","metadata":{"id":"1eF_2o03fJCj"},"source":["# 데이터로드"]},{"cell_type":"markdown","metadata":{"id":"RNWLrmTw_9Mr"},"source":["박은정님의 네이버 영화리뷰 감정분석 데이터를 Github에서 다운로드 합니다. 아래와 같이 nsmc 디렉토리에 있는 ratings_train.txt와 ratings_test.txt를 사용하겠습니다. \n","<br>\n","<br>\n","<br>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":238,"status":"ok","timestamp":1665244639246,"user":{"displayName":"배한진","userId":"03474339660030161101"},"user_tz":-540},"id":"nCKSDHcXTiKn","outputId":"fd1d462e-4cba-43a2-9e35-fae33b782ca9"},"outputs":[{"output_type":"stream","name":"stdout","text":["total 38174\n","drwx------ 2 root root     4096 Oct  6 04:16 code\n","drwx------ 8 root root     4096 Oct  6 04:16 .git\n","-rw------- 1 root root  4893335 Oct  6 04:16 ratings_test.txt\n","-rw------- 1 root root 14628807 Oct  6 04:16 ratings_train.txt\n","-rw------- 1 root root 19515078 Oct  6 04:16 ratings.txt\n","drwx------ 2 root root     4096 Oct  6 04:16 raw\n","-rw------- 1 root root     2596 Oct  6 04:16 README.md\n","-rw------- 1 root root    36746 Oct  6 04:17 synopses.json\n"]}],"source":["# 디렉토리의 파일 목록\n","!ls nsmc -la"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1672,"status":"ok","timestamp":1665255396214,"user":{"displayName":"배한진","userId":"03474339660030161101"},"user_tz":-540},"id":"0LPEdb2tWfIU","outputId":"1359c041-791e-4436-f4ad-1fbe096d676a"},"outputs":[{"output_type":"stream","name":"stdout","text":["(19866, 2)\n","(1000, 2)\n"]}],"source":["# 판다스로 훈련셋과 테스트셋 데이터 로드\n","data = pd.read_excel('/content/drive/MyDrive/github_colab/anireview_blog_pr/ml/ml_data/training.xlsx')\n","train = data[:-1000]\n","test = data[-1000:]\n","\n","print(train.shape)\n","print(test.shape)"]},{"cell_type":"markdown","metadata":{"id":"8Cl0j7TZBoeL"},"source":["훈련셋 150,000개와 테스트셋 50,000개의 데이터가 존재합니다.\n","<br>\n","<br>\n","<br>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":442},"executionInfo":{"elapsed":275,"status":"ok","timestamp":1665255403053,"user":{"displayName":"배한진","userId":"03474339660030161101"},"user_tz":-540},"id":"tejY9ZhABYWl","outputId":"a051a869-1487-44f1-a032-98b2fcd80e1d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                            document  label\n","0                                        정말 친절하고 좋아요    1.0\n","1                               친절해요\\n발톱깍이고 황태간식 갔어요    1.0\n","2                                      선생님 너무 친절하세요~    1.0\n","3                      선생님이 항상 친절하시고 자세히 설명해주셔서 좋아요.    1.0\n","4                                                  굳    1.0\n","5                                                좋아요    1.0\n","6  원장님이 무척 고양이를 사랑하는 병원!  다만 고양이들에게 미움받으셔서 맘아프시다고...    1.0\n","7                                               친절해요    1.0\n","8  의사 선생님께서 친절하시고 과도하게 진료는 안보시는것 같아요 가까운데 좋은 병원이 ...    1.0\n","9                                           친절해요👍🏻 🐈    1.0"],"text/html":["\n","  <div id=\"df-55c9186f-f68c-41a3-a3e3-e5a6148d2119\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>document</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>정말 친절하고 좋아요</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>친절해요\\n발톱깍이고 황태간식 갔어요</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>선생님 너무 친절하세요~</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>선생님이 항상 친절하시고 자세히 설명해주셔서 좋아요.</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>굳</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>좋아요</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>원장님이 무척 고양이를 사랑하는 병원!  다만 고양이들에게 미움받으셔서 맘아프시다고...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>친절해요</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>의사 선생님께서 친절하시고 과도하게 진료는 안보시는것 같아요 가까운데 좋은 병원이 ...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>친절해요👍🏻 🐈</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-55c9186f-f68c-41a3-a3e3-e5a6148d2119')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-55c9186f-f68c-41a3-a3e3-e5a6148d2119 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-55c9186f-f68c-41a3-a3e3-e5a6148d2119');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":95}],"source":["# 훈련셋의 앞부분 출력\n","train.head(10)"]},{"cell_type":"markdown","metadata":{"id":"hZ12coTLB7H3"},"source":["id는 회원정보, document는 리뷰 문장입니다. label이 0이면 부정, 1이면 긍정으로 분류됩니다. id는 사용하지 않기 때문에 document와 label만 추출하겠습니다. "]},{"cell_type":"markdown","metadata":{"id":"ElQPlCUxfc3y"},"source":["# 전저리 - 훈련셋"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":251,"status":"ok","timestamp":1665255406307,"user":{"displayName":"배한진","userId":"03474339660030161101"},"user_tz":-540},"id":"2GoESQ0jbybJ","outputId":"3cb9ff1b-f954-470f-cb2c-45d3eafb9434"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0                                          정말 친절하고 좋아요\n","1                                 친절해요\\n발톱깍이고 황태간식 갔어요\n","2                                        선생님 너무 친절하세요~\n","3                        선생님이 항상 친절하시고 자세히 설명해주셔서 좋아요.\n","4                                                    굳\n","5                                                  좋아요\n","6    원장님이 무척 고양이를 사랑하는 병원!  다만 고양이들에게 미움받으셔서 맘아프시다고...\n","7                                                 친절해요\n","8    의사 선생님께서 친절하시고 과도하게 진료는 안보시는것 같아요 가까운데 좋은 병원이 ...\n","9                                             친절해요👍🏻 🐈\n","Name: document, dtype: object"]},"metadata":{},"execution_count":96}],"source":["# 리뷰 문장 추출\n","sentences = train['document']\n","sentences[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":255,"status":"ok","timestamp":1665255408259,"user":{"displayName":"배한진","userId":"03474339660030161101"},"user_tz":-540},"id":"8KkJZvhccRUJ","outputId":"7bb51355-3648-4029-dbd8-507e3980dd89"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS] 정말 친절하고 좋아요 [SEP]',\n"," '[CLS] 친절해요\\n발톱깍이고 황태간식 갔어요 [SEP]',\n"," '[CLS] 선생님 너무 친절하세요~ [SEP]',\n"," '[CLS] 선생님이 항상 친절하시고 자세히 설명해주셔서 좋아요. [SEP]',\n"," '[CLS] 굳 [SEP]',\n"," '[CLS] 좋아요 [SEP]',\n"," '[CLS] 원장님이 무척 고양이를 사랑하는 병원!  다만 고양이들에게 미움받으셔서 맘아프시다고해요,\\n과도한 진료 안보시고 꼭 필요한것만 맘대디에게 묻고하시고 애기들을 잘어르고 달래서 진료하셔요ㅜ손에 할큄자국과 물리신 자국 범벅이라 맴찢,,,ㅜ다만 원장님이 언제 쉬시는지 걱정,,근처에 친절한 병원 애기 잘봐주시는 병원이라 맘놓고 병원에 데려가도될것같아요,  또 환자가 많아서 약간 기다림은 있지만 정말 잘봐주시는병원, 마스코트 장군이도 넘 스윗하게 방겨줘요,  애기 문제있음 꼭 여기만 데려갈거에욥!! 건강검진 진짜 잘봐주니 저만갈거에욥!!(뿌듯) [SEP]',\n"," '[CLS] 친절해요 [SEP]',\n"," '[CLS] 의사 선생님께서 친절하시고 과도하게 진료는 안보시는것 같아요 가까운데 좋은 병원이 생겨서 좋습니다 👍 [SEP]',\n"," '[CLS] 친절해요👍🏻 🐈 [SEP]']"]},"metadata":{},"execution_count":97}],"source":["# BERT의 입력 형식에 맞게 변환\n","sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n","sentences[:10]"]},{"cell_type":"markdown","metadata":{"id":"NeXrqpRaBjOX"},"source":["![대체 텍스트](https://mino-park7.github.io/images/2019/02/bert-input-representation.png)\n","\n","BERT의 입력은 위의 그림과 같은 형식입니다. Classification을 뜻하는 [CLS] 심볼이 제일 앞에 삽입됩니다. 파인튜닝시 출력에서 이 위치의 값을 사용하여 분류를 합니다. [SEP]은 Seperation을 가리키는데, 두 문장을 구분하는 역할을 합니다. 이 예제에서는 문장이 하나이므로 [SEP]도 하나만 넣습니다.\n","<br>\n","<br>\n","<br>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":239,"status":"ok","timestamp":1665255410985,"user":{"displayName":"배한진","userId":"03474339660030161101"},"user_tz":-540},"id":"7hBblIVQcXJR","outputId":"fdd74b7d-b6c3-4d08-a623-7da21a135304"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1., 1., 1., ..., 1., 1., 1.])"]},"metadata":{},"execution_count":98}],"source":["# 라벨 추출\n","labels = train['label'].values\n","labels"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4391,"status":"ok","timestamp":1665255416401,"user":{"displayName":"배한진","userId":"03474339660030161101"},"user_tz":-540},"id":"PwEplfDvcnZG","outputId":"30c6b1fc-d0f9-4b29-d08c-4fb13ad6af03"},"outputs":[{"output_type":"stream","name":"stdout","text":["[CLS] 정말 친절하고 좋아요 [SEP]\n","['[CLS]', '정', '##말', '친', '##절', '##하고', '좋', '##아', '##요', '[SEP]']\n"]}],"source":["# BERT의 토크나이저로 문장을 토큰으로 분리\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","print (sentences[0])\n","print (tokenized_texts[0])"]},{"cell_type":"markdown","metadata":{"id":"gV6SRXmlTMjr"},"source":["BERT는 형태소분석으로 토큰을 분리하지 않습니다. WordPiece라는 통계적인 방식을 사용합니다. 한 단어내에서 자주 나오는 글자들을 붙여서 하나의 토큰으로 만듭니다. 이렇게 하면 언어에 상관없이 토큰을 생성할 수 있다는 장점이 있습니다. 또한 신조어 같이 사전에 없는 단어를 처리하기도 좋습니다. \n","\n","위의 결과에서 ## 기호는 앞 토큰과 이어진다는 표시입니다. 토크나이저는 여러 언어의 데이터를 기반으로 만든 'bert-base-multilingual-cased'를 사용합니다. 그래서 한글도 처리가 가능합니다.\n","<br>\n","<br>\n","<br>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":645,"status":"ok","timestamp":1665255419065,"user":{"displayName":"배한진","userId":"03474339660030161101"},"user_tz":-540},"id":"VJ76KiP_dLn-","outputId":"adc34aa7-b0ae-438a-cb58-a8505473a04a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  101,  9670, 89523,  9781, 58931, 12453,  9685, 16985, 48549,\n","         102,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0])"]},"metadata":{},"execution_count":100}],"source":["# 입력 토큰의 최대 시퀀스 길이\n","MAX_LEN = 128\n","\n","# 토큰을 숫자 인덱스로 변환\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","\n","# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","input_ids[0]"]},{"cell_type":"markdown","metadata":{"id":"kHI53Jt8VrY8"},"source":["보통 딥러닝 모델에는 토큰 자체를 입력으로 넣을 수 없습니다. 임베딩 레이어에는 토큰을 숫자로 된 인덱스로 변환하여 사용합니다. BERT의 토크나이저는 {단어토큰:인덱스}로 구성된 단어사전을 가지고 있습니다. 이를 참조하여 토큰을 인덱스로 바꿔줍니다.\n","<br>\n","<br>\n","<br>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2030,"status":"ok","timestamp":1665255423223,"user":{"displayName":"배한진","userId":"03474339660030161101"},"user_tz":-540},"id":"pKfL8SotdVaW","outputId":"60ee4129-82de-44e8-dfc7-faf0f4ace1ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"]}],"source":["# 어텐션 마스크 초기화\n","attention_masks = []\n","\n","# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n","# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n","for seq in input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    attention_masks.append(seq_mask)\n","\n","print(attention_masks[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":245,"status":"ok","timestamp":1665255428243,"user":{"displayName":"배한진","userId":"03474339660030161101"},"user_tz":-540},"id":"1f5Vq3-7eNKH","outputId":"5607bdb1-9621-438e-f4d0-bce251f15993"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([  101,  9959, 14871,  8848, 12945, 33188, 48345,   102,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0])\n","tensor(1., dtype=torch.float64)\n","tensor([1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0.])\n","tensor([  101,  9685, 16985, 48549,   198,   102,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0])\n","tensor(1., dtype=torch.float64)\n","tensor([1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0.])\n"]}],"source":["# 훈련셋과 검증셋으로 분리\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\n","                                                                                    labels, \n","                                                                                    random_state=2018, \n","                                                                                    test_size=0.1)\n","\n","# 어텐션 마스크를 훈련셋과 검증셋으로 분리\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, \n","                                                       input_ids,\n","                                                       random_state=2018, \n","                                                       test_size=0.1)\n","\n","# 데이터를 파이토치의 텐서로 변환\n","train_inputs = torch.tensor(train_inputs)\n","train_labels = torch.tensor(train_labels)\n","train_masks = torch.tensor(train_masks)\n","validation_inputs = torch.tensor(validation_inputs)\n","validation_labels = torch.tensor(validation_labels)\n","validation_masks = torch.tensor(validation_masks)\t\t\n","\n","print(train_inputs[0])\n","print(train_labels[0])\n","print(train_masks[0])\n","print(validation_inputs[0])\n","print(validation_labels[0])\n","print(validation_masks[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I3vlyUJuVRo5"},"outputs":[],"source":["# 배치 사이즈\n","batch_size = 32\n","\n","# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n","# 학습시 배치 사이즈 만큼 데이터를 가져옴\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{"id":"zkqUHx51dffp"},"source":["<br>\n","<br>\n","\n","# **전처리 - 테스트셋**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":357,"status":"ok","timestamp":1665255432383,"user":{"displayName":"배한진","userId":"03474339660030161101"},"user_tz":-540},"id":"xgrsNuArd4pj","outputId":"c37916cc-e5b0-4c20-9b3b-83418e198cf4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["19866    마당에 고양이들이 있어서 방문했는데 정말 친절 하세요. \\n민락동 부근 동물병원들에...\n","19867                                          친절 정확 합리적가격\n","19868                                           친절 꼼꼼  좋아요\n","19869     처음방문했는데 선생님들 다 너무친절하세요\\n오늘은 부분만했는데 다음엔 미용해보려구요^^\n","19870                                              친절하세요~~\n","19871                                          친철하고 잘봐주시세요\n","19872                                             친절하고 좋아요\n","19873                                             친절하고 좋네요\n","19874                                       친절하심. 과잉진료 없음 \n","19875                       첫방문  깨끗한 병원 친절하고\\n자세한 설명 추천합니다\n","Name: document, dtype: object"]},"metadata":{},"execution_count":103}],"source":["# 리뷰 문장 추출\n","sentences = test['document']\n","sentences[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":243,"status":"ok","timestamp":1665255434211,"user":{"displayName":"배한진","userId":"03474339660030161101"},"user_tz":-540},"id":"Gtz3QZt9d4pz","outputId":"2f5a53ad-d82b-4db7-da40-208802797c54"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS] 마당에 고양이들이 있어서 방문했는데 정말 친절 하세요. \\n민락동 부근 동물병원들에서 하도많이 당해서 정말 멀리 다녔는데 저희 강아지들도 진료 여기서 보기로 결정 했답니다 강추에요 [SEP]',\n"," '[CLS] 친절 정확 합리적가격 [SEP]',\n"," '[CLS] 친절 꼼꼼  좋아요 [SEP]',\n"," '[CLS] 처음방문했는데 선생님들 다 너무친절하세요\\n오늘은 부분만했는데 다음엔 미용해보려구요^^ [SEP]',\n"," '[CLS] 친절하세요~~ [SEP]',\n"," '[CLS] 친철하고 잘봐주시세요 [SEP]',\n"," '[CLS] 친절하고 좋아요 [SEP]',\n"," '[CLS] 친절하고 좋네요 [SEP]',\n"," '[CLS] 친절하심. 과잉진료 없음  [SEP]',\n"," '[CLS] 첫방문  깨끗한 병원 친절하고\\n자세한 설명 추천합니다 [SEP]']"]},"metadata":{},"execution_count":104}],"source":["# BERT의 입력 형식에 맞게 변환\n","sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n","sentences[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":333,"status":"ok","timestamp":1665255436674,"user":{"displayName":"배한진","userId":"03474339660030161101"},"user_tz":-540},"id":"li8oRajbd4p3","outputId":"77977ad7-8e93-4076-dcf0-593dfda2119f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n","       1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n","       1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n","       1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n","       1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"]},"metadata":{},"execution_count":105}],"source":["# 라벨 추출\n","labels = test['label'].values\n","labels"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":762,"status":"ok","timestamp":1665255441906,"user":{"displayName":"배한진","userId":"03474339660030161101"},"user_tz":-540},"id":"lvpQ49nEd4p6","outputId":"55009742-51d1-4b9f-9079-0709baca83dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["[CLS] 마당에 고양이들이 있어서 방문했는데 정말 친절 하세요. \n","민락동 부근 동물병원들에서 하도많이 당해서 정말 멀리 다녔는데 저희 강아지들도 진료 여기서 보기로 결정 했답니다 강추에요 [SEP]\n","['[CLS]', '마', '##당', '##에', '고', '##양', '##이', '##들이', '있어서', '방', '##문', '##했', '##는데', '정', '##말', '친', '##절', '하', '##세', '##요', '.', '민', '##락', '##동', '부', '##근', '동', '##물', '##병', '##원', '##들', '##에서', '하', '##도', '##많', '##이', '당', '##해서', '정', '##말', '멀', '##리', '다', '##녔', '##는데', '저', '##희', '강', '##아', '##지', '##들', '##도', '진', '##료', '여기서', '보', '##기로', '결', '##정', '했', '##답', '##니다', '강', '##추', '##에', '##요', '[SEP]']\n"]}],"source":["# BERT의 토크나이저로 문장을 토큰으로 분리\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","print (sentences[0])\n","print (tokenized_texts[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":263,"status":"ok","timestamp":1665255443117,"user":{"displayName":"배한진","userId":"03474339660030161101"},"user_tz":-540},"id":"HI9viuAvd4p_","outputId":"f6068882-6d92-4a48-ba2a-f03f9ac3dd24"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([   101,   9246,  21928,  10530,   8888,  37114,  10739,  20173,\n","        90587,   9328,  25934, 119424,  41850,   9670,  89523,   9781,\n","        58931,   9952,  24982,  48549,    119,   9311, 107693,  18778,\n","         9365,  50248,   9095,  29364,  73380,  14279,  27023,  11489,\n","         9952,  12092, 118908,  10739,   9067,  70146,   9670,  89523,\n","         9268,  12692,   9056, 118740,  41850,   9663,  49515,   8853,\n","        16985,  12508,  27023,  12092,   9708,  38688, 108954,   9356,\n","        62675,   8881,  16605,   9965, 118775,  48345,   8853, 119274,\n","        10530,  48549,    102,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0])"]},"metadata":{},"execution_count":107}],"source":["# 입력 토큰의 최대 시퀀스 길이\n","MAX_LEN = 128\n","\n","# 토큰을 숫자 인덱스로 변환\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","\n","# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","input_ids[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":248,"status":"ok","timestamp":1665255445674,"user":{"displayName":"배한진","userId":"03474339660030161101"},"user_tz":-540},"id":"v1NKmP0Fd4qD","outputId":"71e41f77-fa65-485d-803e-7f7391c4b896"},"outputs":[{"output_type":"stream","name":"stdout","text":["[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"]}],"source":["# 어텐션 마스크 초기화\n","attention_masks = []\n","\n","# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n","# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n","for seq in input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    attention_masks.append(seq_mask)\n","\n","print(attention_masks[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":259,"status":"ok","timestamp":1665255448036,"user":{"displayName":"배한진","userId":"03474339660030161101"},"user_tz":-540},"id":"RIkaYCGbd4qG","outputId":"cdf6067c-ae95-4c65-81d7-b56ec9158d03"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([   101,   9246,  21928,  10530,   8888,  37114,  10739,  20173,  90587,\n","          9328,  25934, 119424,  41850,   9670,  89523,   9781,  58931,   9952,\n","         24982,  48549,    119,   9311, 107693,  18778,   9365,  50248,   9095,\n","         29364,  73380,  14279,  27023,  11489,   9952,  12092, 118908,  10739,\n","          9067,  70146,   9670,  89523,   9268,  12692,   9056, 118740,  41850,\n","          9663,  49515,   8853,  16985,  12508,  27023,  12092,   9708,  38688,\n","        108954,   9356,  62675,   8881,  16605,   9965, 118775,  48345,   8853,\n","        119274,  10530,  48549,    102,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0])\n","tensor(1., dtype=torch.float64)\n","tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0.])\n"]}],"source":["# 데이터를 파이토치의 텐서로 변환\n","test_inputs = torch.tensor(input_ids)\n","test_labels = torch.tensor(labels)\n","test_masks = torch.tensor(attention_masks)\n","\n","print(test_inputs[0])\n","print(test_labels[0])\n","print(test_masks[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7gwdYv1Ad4qK"},"outputs":[],"source":["# 배치 사이즈\n","batch_size = 32\n","\n","# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n","# 학습시 배치 사이즈 만큼 데이터를 가져옴\n","test_data = TensorDataset(test_inputs, test_masks, test_labels)\n","test_sampler = RandomSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{"id":"FBvpU-Hfgcth"},"source":["<br>\n","<br>\n","\n","# **모델 생성**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":249,"status":"ok","timestamp":1665255453575,"user":{"displayName":"배한진","userId":"03474339660030161101"},"user_tz":-540},"id":"heToD1ev0mOg","outputId":"7307fd43-d91f-4486-baa5-04becb2b7af8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found GPU at: /device:GPU:0\n"]}],"source":["# GPU 디바이스 이름 구함\n","device_name = tf.test.gpu_device_name()\n","\n","# GPU 디바이스 이름 검사\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":251,"status":"ok","timestamp":1665255455861,"user":{"displayName":"배한진","userId":"03474339660030161101"},"user_tz":-540},"id":"f6enIxvt1FB2","outputId":"89c8b3fc-943c-4d64-a8ad-ebf54a2a2328"},"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"]}],"source":["# 디바이스 설정\n","if torch.cuda.is_available():    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","else:\n","    device = torch.device(\"cpu\")\n","    print('No GPU available, using the CPU instead.')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":250,"status":"ok","timestamp":1665255464095,"user":{"displayName":"배한진","userId":"03474339660030161101"},"user_tz":-540},"id":"b9oh202UmXR7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0381e0a3-87a7-41b2-9db2-42ebb31773f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}],"source":["gpu = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(gpu)\n","# for reproducibility\n","torch.manual_seed(777)\n","if gpu == 'cuda':\n","    torch.cuda.manual_seed_all(777)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MS2MXSiLg5zC","executionInfo":{"status":"ok","timestamp":1665255472879,"user_tz":-540,"elapsed":2783,"user":{"displayName":"배한진","userId":"03474339660030161101"}},"outputId":"d938cb6b-62b4-4dfb-9e94-e4ed7afd1742"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":114}],"source":["# 분류를 위한 BERT 모델 생성\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n","model.cuda()"]},{"cell_type":"markdown","metadata":{"id":"RlszxDr6h8GP"},"source":["![대체 텍스트](http://www.mccormickml.com/assets/BERT/padding_and_mask.png)\n","\n","사전훈련된 BERT는 다양한 문제로 전이학습이 가능합니다. 여기서는 위의 그림과 같이 한 문장을 분류하는 방법을 사용합니다. 영화리뷰 문장이 입력으로 들어가면, 긍정/부정으로 구분합니다. 모델의 출력에서 [CLS] 위치인 첫 번째 토큰에 새로운 레이어를 붙여서 파인튜닝을 합니다. Huggning Face는 BertForSequenceClassification() 함수를 제공하기 때문에 쉽게 구현할 수 있습니다.\n","<br>\n","<br>\n","<br>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":269,"status":"ok","timestamp":1665255477611,"user":{"displayName":"배한진","userId":"03474339660030161101"},"user_tz":-540},"id":"ZIdfbLTuWmxk","outputId":"33a5bd1e-6141-4615-a91b-80cf565a2120"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]}],"source":["# 옵티마이저 설정\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # 학습률\n","                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n","                )\n","\n","# 에폭수\n","epochs = 4\n","\n","# 총 훈련 스텝 : 배치반복 횟수 * 에폭\n","total_steps = len(train_dataloader) * epochs\n","\n","# 처음에 학습률을 조금씩 변화시키는 스케줄러 생성\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0,\n","                                            num_training_steps = total_steps)"]},{"cell_type":"markdown","metadata":{"id":"gzCHV_ghj7DM"},"source":["<br>\n","<br>\n","\n","# **모델 학습**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S0-p6pPVXCRe"},"outputs":[],"source":["# 정확도 계산 함수\n","def flat_accuracy(preds, labels):\n","    \n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FJXISnJzCdLM"},"outputs":[],"source":["# 시간 표시 함수\n","def format_time(elapsed):\n","\n","    # 반올림\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # hh:mm:ss으로 형태 변경\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"muU2kS2GCh4y","outputId":"aac7b264-752d-4c25-d46c-1e86298a6896","executionInfo":{"status":"ok","timestamp":1665257141065,"user_tz":-540,"elapsed":477048,"user":{"displayName":"배한진","userId":"03474339660030161101"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======== Epoch 1 / 4 ========\n","Training...\n","\n","  Average training loss: 0.01\n","  Training epcoh took: 0:01:47\n","\n","Running Validation...\n","  Accuracy: 0.99\n","  Validation took: 0:00:07\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","\n","  Average training loss: 0.01\n","  Training epcoh took: 0:01:52\n","\n","Running Validation...\n","  Accuracy: 0.99\n","  Validation took: 0:00:08\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","\n","  Average training loss: 0.01\n","  Training epcoh took: 0:01:53\n","\n","Running Validation...\n","  Accuracy: 0.99\n","  Validation took: 0:00:07\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","\n","  Average training loss: 0.01\n","  Training epcoh took: 0:01:55\n","\n","Running Validation...\n","  Accuracy: 0.99\n","  Validation took: 0:00:07\n","\n","Training complete!\n"]}],"source":["# 재현을 위해 랜덤시드 고정\n","seed_val = 42\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# 그래디언트 초기화\n","model.zero_grad()\n","\n","# 에폭만큼 반복\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # 시작 시간 설정\n","    t0 = time.time()\n","\n","    # 로스 초기화\n","    total_loss = 0\n","\n","    # 훈련모드로 변경\n","    model.train()\n","        \n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for step, batch in enumerate(train_dataloader):\n","        # 경과 정보 표시\n","        if step % 500 == 0 and not step == 0:\n","            elapsed = format_time(time.time() - t0)\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # 배치를 GPU에 넣음\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # 배치에서 데이터 추출\n","        b_input_ids, b_input_mask, b_labels = batch\n","\n","        # Forward 수행                \n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask, \n","                        labels=b_labels)\n","        \n","        # 로스 구함\n","        loss = outputs[0]\n","\n","        # 총 로스 계산\n","        total_loss += loss.item()\n","\n","        # Backward 수행으로 그래디언트 계산\n","        loss.backward()\n","\n","        # 그래디언트 클리핑\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # 그래디언트를 통해 가중치 파라미터 업데이트\n","        optimizer.step()\n","\n","        # 스케줄러로 학습률 감소\n","        scheduler.step()\n","\n","        # 그래디언트 초기화\n","        model.zero_grad()\n","\n","    # 평균 로스 계산\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    #시작 시간 설정\n","    t0 = time.time()\n","\n","    # 평가모드로 변경\n","    model.eval()\n","\n","    # 변수 초기화\n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for batch in validation_dataloader:\n","        # 배치를 GPU에 넣음\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # 배치에서 데이터 추출\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        # 그래디언트 계산 안함\n","        with torch.no_grad():     \n","            # Forward 수행\n","            outputs = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask)\n","        \n","        # 출력 로짓 구함\n","        logits = outputs[0]\n","\n","        # CPU로 데이터 이동\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # 출력 로짓과 라벨을 비교하여 정확도 계산\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        eval_accuracy += tmp_eval_accuracy\n","        nb_eval_steps += 1\n","    model_save_name = 'review_cl_md.pt'\n","    model.save_pretrained(F\"/content/drive/MyDrive/github_colab/anireview_blog_pr/ml/{model_save_name}\") \n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","\n","print(\"\")\n","print(\"Training complete!\")"]},{"cell_type":"markdown","metadata":{"id":"hxlXEBA0WefL"},"source":["에폭마다 훈련셋과 검증셋을 반복하여 학습을 수행합니다. "]},{"cell_type":"markdown","metadata":{"id":"6BVbl4Zjatzn"},"source":["<br>\n","<br>\n","\n","# **테스트셋 평가**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7708,"status":"ok","timestamp":1665257167638,"user":{"displayName":"배한진","userId":"03474339660030161101"},"user_tz":-540},"id":"c5KHb6RkbHdj","outputId":"817df378-e8c7-4166-e8c5-93f16acaa189"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Accuracy: 0.98\n","Test took: 0:00:07\n"]}],"source":["#시작 시간 설정\n","t0 = time.time()\n","\n","# 평가모드로 변경\n","model.eval()\n","\n","# 변수 초기화\n","eval_loss, eval_accuracy = 0, 0\n","nb_eval_steps, nb_eval_examples = 0, 0\n","\n","# 데이터로더에서 배치만큼 반복하여 가져옴\n","for step, batch in enumerate(test_dataloader):\n","    # 경과 정보 표시\n","    if step % 100 == 0 and not step == 0:\n","        elapsed = format_time(time.time() - t0)\n","        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n","\n","    # 배치를 GPU에 넣음\n","    batch = tuple(t.to(device) for t in batch)\n","    \n","    # 배치에서 데이터 추출\n","    b_input_ids, b_input_mask, b_labels = batch\n","    \n","    # 그래디언트 계산 안함\n","    with torch.no_grad():     \n","        # Forward 수행\n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask)\n","    \n","    # 출력 로짓 구함\n","    logits = outputs[0]\n","\n","    # CPU로 데이터 이동\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","    \n","    # 출력 로짓과 라벨을 비교하여 정확도 계산\n","    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","    eval_accuracy += tmp_eval_accuracy\n","    nb_eval_steps += 1\n","\n","print(\"\")\n","print(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","print(\"Test took: {:}\".format(format_time(time.time() - t0)))"]},{"cell_type":"markdown","metadata":{"id":"DbsNMA8Idc3K"},"source":["테스트셋의 정확도가 87%입니다. <BERT 톺아보기> 블로그에서는 같은 데이터로 88.7%를 달성하였습니다. 거기서는 한글 코퍼스로 사전훈련을 하여 새로운 모델을 만들었습니다. 반면에 우리는 BERT의 기본 모델인 bert-base-multilingual-cased를 사용했기 때문에 더 성능이 낮은 것 같습니다."]},{"cell_type":"markdown","metadata":{"id":"U7SzL1IBe1Dm"},"source":["<br>\n","<br>\n","\n","# **새로운 문장 테스트**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tb4v_VfEfGQB"},"outputs":[],"source":["# 입력 데이터 변환\n","def convert_input_data(sentences):\n","\n","    # BERT의 토크나이저로 문장을 토큰으로 분리\n","    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","    # 입력 토큰의 최대 시퀀스 길이\n","    MAX_LEN = 128\n","\n","    # 토큰을 숫자 인덱스로 변환\n","    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","    \n","    # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n","    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","    # 어텐션 마스크 초기화\n","    attention_masks = []\n","\n","    # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n","    # 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n","    for seq in input_ids:\n","        seq_mask = [float(i>0) for i in seq]\n","        attention_masks.append(seq_mask)\n","\n","    # 데이터를 파이토치의 텐서로 변환\n","    inputs = torch.tensor(input_ids)\n","    masks = torch.tensor(attention_masks)\n","\n","    return inputs, masks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C12NL1Fvgv4E"},"outputs":[],"source":["# 문장 테스트\n","def test_sentences(sentences):\n","\n","    # 평가모드로 변경\n","    model.eval()\n","\n","    # 문장을 입력 데이터로 변환\n","    inputs, masks = convert_input_data(sentences)\n","\n","    # 데이터를 GPU에 넣음\n","    b_input_ids = inputs.to(device)\n","    b_input_mask = masks.to(device)\n","            \n","    # 그래디언트 계산 안함\n","    with torch.no_grad():     \n","        # Forward 수행\n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask)\n","\n","    # 출력 로짓 구함\n","    logits = outputs[0]\n","\n","    # CPU로 데이터 이동\n","    logits = logits.detach().cpu().numpy()\n","\n","    return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":240,"status":"ok","timestamp":1665256324559,"user":{"displayName":"배한진","userId":"03474339660030161101"},"user_tz":-540},"id":"ZQezr0tljJlM","outputId":"f976743f-03e6-4748-b8d1-93d949b08651"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 2.6667283 -2.5631375]]\n","0\n"]}],"source":["logits = test_sentences(['비싸고 모두 다 별로'])\n","\n","print(logits)\n","print(np.argmax(logits))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":241,"status":"ok","timestamp":1665256328835,"user":{"displayName":"배한진","userId":"03474339660030161101"},"user_tz":-540},"id":"-9MQ0SK0jofN","outputId":"bfc779b7-b110-4d09-971c-2dd1859a0da2"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 2.5667222 -2.674333 ]]\n","0\n"]}],"source":["logits = test_sentences(['약값이 비싸요'])\n","\n","print(logits)\n","print(np.argmax(logits))"]},{"cell_type":"markdown","metadata":{"id":"H5mANMwKkA0D"},"source":["학습한 모델을 가지고 실제 문장을 넣어봤습니다. 출력 로짓은 소프트맥스가 적용되지 않은 상태입니다. argmax로 더 높은 값의 위치를 라벨로 설정하면 됩니다. 0은 부정, 1은 긍정입니다. 위와 같이 새로운 문장에도 잘 분류를 하고 있습니다.\n","<br>\n","<br>\n","<br>"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[],"authorship_tag":"ABX9TyMDkIJeuJ5VCXj5a8G/zQYD"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}